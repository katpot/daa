{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dynamic Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def GETNEXTTOKEN():\n",
    "    global token_stream\n",
    "    if len(token_stream) > 0:\n",
    "        next_token = token_stream.pop(0)  # Get the next token from the stream\n",
    "        return next_token\n",
    "    else:\n",
    "        return None  # Return None if the token stream is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LMERGEADDTOKEN(ind, Z, token, n=10):\n",
    "    Z[0].add(token)\n",
    "    if len(Z[0]) == n:\n",
    "        i = 0\n",
    "        while True:\n",
    "            if i<len(ind):\n",
    "                if ind[i] != -1:\n",
    "                    Z.append(Z[i].union(ind[i]))\n",
    "                    ind[i] = -1\n",
    "                else:\n",
    "                    ind[i] = Z[i]\n",
    "            else:\n",
    "                ind.append(Z[i])\n",
    "                break\n",
    "        Z = [set()]\n",
    "\n",
    "def LOGARITHMICMERGE(docs):\n",
    "    Z  = [set()]\n",
    "    ind = []\n",
    "    while True:\n",
    "        token = GETNEXTTOKEN()\n",
    "        if token is None:\n",
    "            break\n",
    "        post_list = []\n",
    "        for j in range(len(docs)):\n",
    "            if token in docs[j]:\n",
    "                post_list.append(j+1)\n",
    "        token = (token,tuple(post_list))\n",
    "        LMERGEADDTOKEN(ind, Z, token)\n",
    "    return(heapq.merge(*ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructed index : \n",
      "('angel', (2,))\n",
      "('his', (2,))\n",
      "('told', (1,))\n",
      "('oath', (3,))\n",
      "('o', (1,))\n",
      "('best', (1,))\n",
      "('thine', (3,))\n",
      "('know', (1, 2))\n",
      "('vow', (3,))\n",
      "('soon', (2,))\n",
      "('fiend', (2,))\n",
      "('vapour', (3,))\n",
      "('broke', (3,))\n",
      "('doth', (3,))\n",
      "('be', (1, 2))\n",
      "('vows', (3,))\n",
      "('but', (1, 2, 3))\n",
      "('both', (2,))\n",
      "('cures', (3,))\n",
      "('smiling', (1,))\n",
      "('bad', (2,))\n",
      "('that', (1, 2, 3))\n",
      "('colour', (2,))\n",
      "('fire', (2,))\n",
      "('tempteth', (2,))\n",
      "('gainst', (3,))\n",
      "('female', (2,))\n",
      "('ll', (1,))\n",
      "('being', (2, 3))\n",
      "('goddess', (3,))\n",
      "('fool', (3,))\n",
      "('this', (3,))\n",
      "('persuade', (3,))\n",
      "('soothing', (1,))\n",
      "('shine', (3,))\n",
      "('s', (1, 2))\n",
      "('credit', (1,))\n",
      "('years', (1,))\n",
      "('saint', (2,))\n",
      "('side', (2,))\n",
      "('fair', (2, 3))\n",
      "('is', (1, 2, 3))\n",
      "('and', (1, 2, 3))\n",
      "('vainly', (1,))\n",
      "('swears', (1,))\n",
      "('one', (2,))\n",
      "('outfacing', (1,))\n",
      "('ii', (2,))\n",
      "('tell', (2,))\n",
      "('earth', (3,))\n",
      "('may', (2,))\n",
      "('think', (1,))\n",
      "('comfort', (2,))\n",
      "('broken', (3,))\n",
      "('could', (3,))\n",
      "('each', (2,))\n",
      "('break', (3,))\n",
      "('pride', (2,))\n",
      "('though', (1,))\n",
      "('right', (2,))\n",
      "('our', (1,))\n",
      "('tongue', (1,))\n",
      "('it', (3,))\n",
      "('age', (1,))\n",
      "('am', (1,))\n",
      "('win', (2, 3))\n",
      "('to', (1, 2, 3))\n",
      "('shall', (2,))\n",
      "('till', (2,))\n",
      "('argument', (3,))\n",
      "('smother', (1,))\n",
      "('would', (2,))\n",
      "('whether', (2,))\n",
      "('hold', (3,))\n",
      "('for', (2, 3))\n",
      "('some', (1,))\n",
      "('two', (2,))\n",
      "('habit', (1,))\n",
      "('eye', (3,))\n",
      "('paradise', (3,))\n",
      "('on', (3,))\n",
      "('the', (1, 2, 3))\n",
      "('past', (1,))\n",
      "('in', (1, 2, 3))\n",
      "('lie', (1,))\n",
      "('untutor', (1,))\n",
      "('believe', (1,))\n",
      "('sun', (3,))\n",
      "('suggest', (2,))\n",
      "('spirit', (2,))\n",
      "('corrupt', (2,))\n",
      "('friend', (2,))\n",
      "('suspect', (2,))\n",
      "('prove', (3,))\n",
      "('says', (1,))\n",
      "('rest', (1,))\n",
      "('turn', (2,))\n",
      "('say', (1,))\n",
      "('youth', (1,))\n",
      "('i', (1, 2, 3))\n",
      "('forswore', (3,))\n",
      "('no', (3,))\n",
      "('spirits', (2,))\n",
      "('thinking', (1,))\n",
      "('with', (1, 2))\n",
      "('lies', (1,))\n",
      "('punishment', (3,))\n",
      "('worser', (2,))\n",
      "('world', (1, 3))\n",
      "('my', (1, 2, 3))\n",
      "('exhale', (3,))\n",
      "('disgrace', (3,))\n",
      "('so', (3,))\n",
      "('might', (1,))\n",
      "('deserve', (3,))\n",
      "('despair', (2,))\n",
      "('unskilful', (1,))\n",
      "('speaking', (1,))\n",
      "('from', (2,))\n",
      "('hell', (2,))\n",
      "('ill', (1, 2))\n",
      "('wooing', (2,))\n",
      "('out', (2,))\n",
      "('iii', (3,))\n",
      "('still', (2,))\n",
      "('faults', (1,))\n",
      "('old', (1,))\n",
      "('young', (1,))\n",
      "('what', (3,))\n",
      "('since', (1,))\n",
      "('loves', (1, 2))\n",
      "('not', (1, 2, 3))\n",
      "('guess', (2,))\n",
      "('thou', (3,))\n",
      "('heavenly', (3,))\n",
      "('doubt', (2,))\n",
      "('rhetoric', (3,))\n",
      "('d', (1, 2, 3))\n",
      "('thy', (3,))\n",
      "('do', (1, 2))\n",
      "('made', (1,))\n",
      "('purity', (2,))\n",
      "('mine', (3,))\n",
      "('fault', (3,))\n",
      "('like', (2,))\n",
      "('wherefore', (1,))\n",
      "('love', (1, 3))\n",
      "('therefore', (1,))\n",
      "('if', (3,))\n",
      "('she', (1,))\n",
      "('woman', (2, 3))\n",
      "('a', (1, 2, 3))\n",
      "('earthly', (3,))\n",
      "('perjury', (3,))\n",
      "('whom', (3,))\n",
      "('have', (1, 2))\n",
      "('live', (2,))\n",
      "('all', (3,))\n",
      "('thee', (3,))\n",
      "('then', (3,))\n",
      "('when', (1,))\n",
      "('devil', (2,))\n",
      "('thus', (1,))\n",
      "('was', (3,))\n",
      "('an', (3,))\n",
      "('false', (1, 3))\n",
      "('evil', (2,))\n",
      "('directly', (2,))\n",
      "('good', (2,))\n",
      "('did', (3,))\n",
      "('heart', (3,))\n",
      "('grace', (3,))\n",
      "('will', (3,))\n",
      "('breath', (3,))\n",
      "('man', (2,))\n",
      "('yet', (2,))\n",
      "('of', (1, 2, 3))\n",
      "('another', (2,))\n",
      "('although', (1,))\n",
      "('wise', (3,))\n",
      "('gain', (3,))\n",
      "('thinks', (1,))\n",
      "('better', (2,))\n",
      "('by', (3,))\n",
      "('truth', (1, 2))\n",
      "('me', (1, 2, 3))\n",
      "('forgeries', (1,))\n",
      "('her', (1, 2))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "docs = []\n",
    "terms = set()\n",
    "l = ['documents/'+str(i)+'.txt' for i in range(1,4)]\n",
    "\n",
    "for i in l:\n",
    "    file = open(i,'r').read().lower()\n",
    "    content = re.sub(r\"[^a-zA-Z0-9]\",\" \",file).split()\n",
    "    terms.update(content)\n",
    "    docs.append(set(content))\n",
    "\n",
    "token_stream = list(terms)\n",
    "print(\"constructed index : \")\n",
    "for i in LOGARITHMICMERGE(docs):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
